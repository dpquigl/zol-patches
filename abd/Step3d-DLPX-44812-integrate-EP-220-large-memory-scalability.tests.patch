From 2f48b2be71e7f65fd640fad4b0f1293da8740092 Mon Sep 17 00:00:00 2001
From: Dan Kimmel <dan.kimmel@delphix.com>
Date: Mon, 30 May 2016 19:07:37 -0700
Subject: [PATCH 5/6] DLPX-44812 integrate EP-220 large memory scalability

---
 usr/src/test/zfs-tests/cmd/memory_balloon/Makefile |  22 +
 .../zfs-tests/cmd/memory_balloon/memory_balloon.c  | 103 +++
 usr/src/test/zfs-tests/include/default.cfg         |   1 +
 .../test/zfs-tests/runfiles/perf-regression.run    |   8 +-
 usr/src/test/zfs-tests/tests/perf/perf.shlib       |  14 +-
 .../tests/perf/regression/random_reads.ksh         |   3 +-
 .../tests/perf/regression/random_readwrite.ksh     |   3 +-
 .../tests/perf/regression/random_writes.ksh        |   3 +-
 .../tests/perf/regression/sequential_reads.ksh     |   3 +-
 .../regression/sequential_reads_arc_cached.ksh     |  78 ++
 .../sequential_reads_arc_cached_clone.ksh          |  94 ++
 .../perf/regression/sequential_reads_cached.ksh    |  77 --
 .../regression/sequential_reads_cached_clone.ksh   |  93 --
 .../regression/sequential_reads_dbuf_cached.ksh    |  82 ++
 .../tests/perf/regression/sequential_writes.ksh    |   3 +-
 .../test/zfs-tests/tests/perf/scripts/profile.d    |  37 +
 16 files changed, 444 insertions(+), 180 deletions(-)
 create mode 100644 usr/src/test/zfs-tests/cmd/memory_balloon/Makefile
 create mode 100644 usr/src/test/zfs-tests/cmd/memory_balloon/memory_balloon.c
 create mode 100644 usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_arc_cached.ksh
 create mode 100644 usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_arc_cached_clone.ksh
 delete mode 100644 usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_cached.ksh
 delete mode 100644 usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_cached_clone.ksh
 create mode 100644 usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_dbuf_cached.ksh
 create mode 100644 usr/src/test/zfs-tests/tests/perf/scripts/profile.d

diff --git a/usr/src/test/zfs-tests/cmd/memory_balloon/Makefile b/usr/src/test/zfs-tests/cmd/memory_balloon/Makefile
new file mode 100644
index 0000000..df1d370
--- /dev/null
+++ b/usr/src/test/zfs-tests/cmd/memory_balloon/Makefile
@@ -0,0 +1,22 @@
+#
+# This file and its contents are supplied under the terms of the
+# Common Development and Distribution License ("CDDL"), version 1.0.
+# You may only use this file in accordance with the terms of version
+# 1.0 of the CDDL.
+#
+# A full copy of the text of the CDDL should have accompanied this
+# source.  A copy of the CDDL is also available via the Internet at
+# http://www.illumos.org/license/CDDL.
+#
+
+#
+# Copyright (c) 2016 by Delphix. All rights reserved.
+#
+
+PROG = memory_balloon
+
+include $(SRC)/cmd/Makefile.cmd
+
+LINTFLAGS += -erroff=E_FUNC_SET_NOT_USED
+
+include ../Makefile.subdirs
diff --git a/usr/src/test/zfs-tests/cmd/memory_balloon/memory_balloon.c b/usr/src/test/zfs-tests/cmd/memory_balloon/memory_balloon.c
new file mode 100644
index 0000000..958f6e6
--- /dev/null
+++ b/usr/src/test/zfs-tests/cmd/memory_balloon/memory_balloon.c
@@ -0,0 +1,103 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright (c) 2016 by Delphix. All rights reserved.
+ */
+
+/*
+ * Steal memory from the kernel, forcing the ARC to decrease in size, and hold
+ * it until the process receives a signal.
+ */
+
+#include <stdio.h>
+#include <sys/types.h>
+#include <sys/shm.h>
+#include <strings.h>
+#include <stdlib.h>
+#include <unistd.h>
+#include <errno.h>
+
+static void
+usage(char *progname)
+{
+	(void) fprintf(stderr, "Usage: %s -f <bytes>\n", progname);
+	exit(1);
+}
+
+static void
+fail(char *err, int rval)
+{
+	perror(err);
+	exit(rval);
+}
+
+static void
+daemonize(void)
+{
+	pid_t	pid;
+
+	if ((pid = fork()) < 0) {
+		fail("fork", 1);
+	} else if (pid != 0) {
+		(void) fprintf(stdout, "%ld\n", pid);
+		exit(0);
+	}
+
+	(void) setsid();
+	(void) close(0);
+	(void) close(1);
+	(void) close(2);
+}
+
+int
+main(int argc, char *argv[])
+{
+	int		c;
+	boolean_t	fflag = B_FALSE;
+	char		*prog = argv[0];
+	long long	size;
+	char		*stroll_leftovers;
+	int		shm_id;
+	void		*shm_attached;
+
+	while ((c = getopt(argc, argv, "f")) != -1) {
+		switch (c) {
+		/* Run in the foreground */
+		case 'f':
+			fflag = B_TRUE;
+			break;
+		default:
+			usage(prog);
+		}
+	}
+
+	argc -= optind;
+	argv += optind;
+
+	if (argc != 1)
+		usage(prog);
+	size = strtoll(argv[0], &stroll_leftovers, 10);
+	if (size <= 0)
+		fail("invalid size in bytes", 1);
+
+	if ((shm_id = shmget(IPC_PRIVATE, size, IPC_CREAT|IPC_EXCL)) == -1)
+		fail("shmget", 1);
+	if ((shm_attached = shmat(shm_id, NULL, SHM_SHARE_MMU)) == (void *)-1)
+		fail("shmat", 1);
+
+	if (fflag == B_FALSE)
+		daemonize();
+	(void) pause();
+
+	/* NOTREACHED */
+	return (0);
+}
diff --git a/usr/src/test/zfs-tests/include/default.cfg b/usr/src/test/zfs-tests/include/default.cfg
index bab2e9b..df404c3 100644
--- a/usr/src/test/zfs-tests/include/default.cfg
+++ b/usr/src/test/zfs-tests/include/default.cfg
@@ -49,6 +49,7 @@ export FILE_TRUNC="/opt/zfs-tests/bin/file_trunc"
 export FILE_WRITE="/opt/zfs-tests/bin/file_write"
 export GETHOLES="/opt/zfs-tests/bin/getholes"
 export LARGEST_FILE="/opt/zfs-tests/bin/largest_file"
+export MEMORY_BALLOON="/opt/zfs-tests/bin/memory_balloon"
 export MKBUSY="/opt/zfs-tests/bin/mkbusy"
 export MKHOLES="/opt/zfs-tests/bin/mkholes"
 export MKTREE="/opt/zfs-tests/bin/mktree"
diff --git a/usr/src/test/zfs-tests/runfiles/perf-regression.run b/usr/src/test/zfs-tests/runfiles/perf-regression.run
index 0095931..dbb30f0 100644
--- a/usr/src/test/zfs-tests/runfiles/perf-regression.run
+++ b/usr/src/test/zfs-tests/runfiles/perf-regression.run
@@ -10,7 +10,7 @@
 #
 
 #
-# Copyright (c) 2015 by Delphix. All rights reserved.
+# Copyright (c) 2015, 2016 by Delphix. All rights reserved.
 #
 
 [DEFAULT]
@@ -24,7 +24,7 @@ post = cleanup
 outputdir = /var/tmp/test_results
 
 [/opt/zfs-tests/tests/perf/regression]
-tests = ['sequential_writes', 'sequential_reads', 'sequential_reads_cached',
-    'sequential_reads_cached_clone', 'random_reads', 'random_writes',
-    'random_readwrite']
+tests = ['sequential_writes', 'sequential_reads', 'sequential_reads_arc_cached',
+    'sequential_reads_arc_cached_clone', 'sequential_reads_dbuf_cached',
+    'random_reads', 'random_writes', 'random_readwrite']
 post =
diff --git a/usr/src/test/zfs-tests/tests/perf/perf.shlib b/usr/src/test/zfs-tests/tests/perf/perf.shlib
index 2b4d043..c3aa5f2 100644
--- a/usr/src/test/zfs-tests/tests/perf/perf.shlib
+++ b/usr/src/test/zfs-tests/tests/perf/perf.shlib
@@ -10,7 +10,7 @@
 #
 
 #
-# Copyright (c) 2015 by Delphix. All rights reserved.
+# Copyright (c) 2015, 2016 by Delphix. All rights reserved.
 #
 
 . $STF_SUITE/include/libtest.shlib
@@ -173,6 +173,18 @@ function get_max_arc_size
 	echo $max_arc_size
 }
 
+function get_max_dbuf_cache_size
+{
+	typeset -l max_dbuf_cache_size=$(dtrace -qn 'BEGIN {
+	    printf("%u\n", `dbuf_cache_max_bytes);
+	    exit(0);
+	}')
+
+	[[ $? -eq 0 ]] || log_fail "get_max_dbuf_cache_size failed"
+
+	echo $max_dbuf_cache_size
+}
+
 # Create a file with some information about how this system is configured.
 function get_system_config
 {
diff --git a/usr/src/test/zfs-tests/tests/perf/regression/random_reads.ksh b/usr/src/test/zfs-tests/tests/perf/regression/random_reads.ksh
index 2395b89..7f30d9e 100644
--- a/usr/src/test/zfs-tests/tests/perf/regression/random_reads.ksh
+++ b/usr/src/test/zfs-tests/tests/perf/regression/random_reads.ksh
@@ -70,7 +70,8 @@ log_must $FIO $FIO_SCRIPTS/mkfiles.fio
 lun_list=$(pool_to_lun_list $PERFPOOL)
 log_note "Collecting backend IO stats with lun list $lun_list"
 export collect_scripts=("$PERF_SCRIPTS/io.d $PERFPOOL $lun_list 1" "io"
-    "$VMSTAT 1" "vmstat" "$MPSTAT 1" "mpstat" "$IOSTAT -xcnz 1" "iostat")
+    "$VMSTAT 1" "vmstat" "$MPSTAT 1" "mpstat" "$IOSTAT -xcnz 1" "iostat"
+    "dtrace -s $PERF_SCRIPTS/profile.d" "profile" "kstat zfs:0 1" "kstat")
 
 log_note "Random reads with $PERF_RUNTYPE settings"
 do_fio_run random_reads.fio $FALSE $TRUE
diff --git a/usr/src/test/zfs-tests/tests/perf/regression/random_readwrite.ksh b/usr/src/test/zfs-tests/tests/perf/regression/random_readwrite.ksh
index be87e43..87c2eda 100644
--- a/usr/src/test/zfs-tests/tests/perf/regression/random_readwrite.ksh
+++ b/usr/src/test/zfs-tests/tests/perf/regression/random_readwrite.ksh
@@ -70,7 +70,8 @@ log_must $FIO $FIO_SCRIPTS/mkfiles.fio
 lun_list=$(pool_to_lun_list $PERFPOOL)
 log_note "Collecting backend IO stats with lun list $lun_list"
 export collect_scripts=("$PERF_SCRIPTS/io.d $PERFPOOL $lun_list 1" "io"
-    "$VMSTAT 1" "vmstat" "$MPSTAT 1" "mpstat" "$IOSTAT -xcnz 1" "iostat")
+    "$VMSTAT 1" "vmstat" "$MPSTAT 1" "mpstat" "$IOSTAT -xcnz 1" "iostat"
+    "dtrace -s $PERF_SCRIPTS/profile.d" "profile" "kstat zfs:0 1" "kstat")
 
 log_note "Random reads and writes with $PERF_RUNTYPE settings"
 do_fio_run random_readwrite.fio $FALSE $TRUE
diff --git a/usr/src/test/zfs-tests/tests/perf/regression/random_writes.ksh b/usr/src/test/zfs-tests/tests/perf/regression/random_writes.ksh
index a4469c3..fb99c1a 100644
--- a/usr/src/test/zfs-tests/tests/perf/regression/random_writes.ksh
+++ b/usr/src/test/zfs-tests/tests/perf/regression/random_writes.ksh
@@ -62,7 +62,8 @@ fi
 lun_list=$(pool_to_lun_list $PERFPOOL)
 log_note "Collecting backend IO stats with lun list $lun_list"
 export collect_scripts=("$PERF_SCRIPTS/io.d $PERFPOOL $lun_list 1" "io"
-    "$VMSTAT 1" "vmstat" "$MPSTAT 1" "mpstat" "$IOSTAT -xcnz 1" "iostat")
+    "$VMSTAT 1" "vmstat" "$MPSTAT 1" "mpstat" "$IOSTAT -xcnz 1" "iostat"
+    "dtrace -s $PERF_SCRIPTS/profile.d" "profile" "kstat zfs:0 1" "kstat")
 
 log_note "Random writes with $PERF_RUNTYPE settings"
 do_fio_run random_writes.fio $TRUE $FALSE
diff --git a/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads.ksh b/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads.ksh
index b04d06c..9d3afad 100644
--- a/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads.ksh
+++ b/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads.ksh
@@ -71,7 +71,8 @@ lun_list=$(pool_to_lun_list $PERFPOOL)
 log_note "Collecting backend IO stats with lun list $lun_list"
 export collect_scripts=("$PERF_SCRIPTS/io.d $PERFPOOL $lun_list 1" "io"
     "$PERF_SCRIPTS/prefetch_io.d $PERFPOOL 1" "prefetch" "$VMSTAT 1" "vmstat"
-    "$MPSTAT 1" "mpstat" "$IOSTAT -xcnz 1" "iostat")
+    "$MPSTAT 1" "mpstat" "$IOSTAT -xcnz 1" "iostat"
+    "dtrace -s $PERF_SCRIPTS/profile.d" "profile" "kstat zfs:0 1" "kstat")
 
 log_note "Sequential reads with $PERF_RUNTYPE settings"
 do_fio_run sequential_reads.fio $FALSE $TRUE
diff --git a/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_arc_cached.ksh b/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_arc_cached.ksh
new file mode 100644
index 0000000..5ae0e15
--- /dev/null
+++ b/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_arc_cached.ksh
@@ -0,0 +1,78 @@
+#!/usr/bin/ksh
+
+#
+# This file and its contents are supplied under the terms of the
+# Common Development and Distribution License ("CDDL"), version 1.0.
+# You may only use this file in accordance with the terms of version
+# 1.0 of the CDDL.
+#
+# A full copy of the text of the CDDL should have accompanied this
+# source.  A copy of the CDDL is also available via the Internet at
+# http://www.illumos.org/license/CDDL.
+#
+
+#
+# Copyright (c) 2015 by Delphix. All rights reserved.
+#
+
+#
+# Description:
+# Trigger fio runs using the sequential_reads job file. The number of runs and
+# data collected is determined by the PERF_* variables. See do_fio_run for
+# details about these variables.
+#
+# The files to read from are created prior to the first fio run, and used
+# for all fio runs. The ARC is not cleared to ensure that all data is cached.
+#
+
+. $STF_SUITE/include/libtest.shlib
+. $STF_SUITE/tests/perf/perf.shlib
+
+function cleanup
+{
+	log_must $ZFS destroy $TESTFS
+}
+
+log_assert "Measure IO stats during sequential read load"
+log_onexit cleanup
+
+export TESTFS=$PERFPOOL/testfs
+recreate_perfpool
+log_must $ZFS create $PERF_FS_OPTS $TESTFS
+
+# Make sure the working set can be cached in the arc. Aim for 1/2 of arc.
+export TOTAL_SIZE=$(($(get_max_arc_size) / 2))
+
+# Variables for use by fio.
+if [[ -n $PERF_REGRESSION_WEEKLY ]]; then
+	export PERF_RUNTIME=${PERF_RUNTIME:-$PERF_RUNTIME_WEEKLY}
+	export PERF_RUNTYPE=${PERF_RUNTYPE:-'weekly'}
+	export PERF_NTHREADS=${PERF_NTHREADS:-'16 64'}
+	export PERF_SYNC_TYPES=${PERF_SYNC_TYPES:-'1'}
+	export PERF_IOSIZES=${PERF_IOSIZES:-'64k 128k 1m'}
+elif [[ -n $PERF_REGRESSION_NIGHTLY ]]; then
+	export PERF_RUNTIME=${PERF_RUNTIME:-$PERF_RUNTIME_NIGHTLY}
+	export PERF_RUNTYPE=${PERF_RUNTYPE:-'nightly'}
+	export PERF_NTHREADS=${PERF_NTHREADS:-'64 128'}
+	export PERF_SYNC_TYPES=${PERF_SYNC_TYPES:-'1'}
+	export PERF_IOSIZES=${PERF_IOSIZES:-'128k 1m'}
+fi
+
+# Layout the files to be used by the read tests. Create as many files as the
+# largest number of threads. An fio run with fewer threads will use a subset
+# of the available files.
+export NUMJOBS=$(get_max $PERF_NTHREADS)
+export FILE_SIZE=$((TOTAL_SIZE / NUMJOBS))
+log_must $FIO $FIO_SCRIPTS/mkfiles.fio
+
+# Set up the scripts and output files that will log performance data.
+lun_list=$(pool_to_lun_list $PERFPOOL)
+log_note "Collecting backend IO stats with lun list $lun_list"
+export collect_scripts=("$PERF_SCRIPTS/io.d $PERFPOOL $lun_list 1" "io"
+    "$PERF_SCRIPTS/prefetch_io.d $PERFPOOL 1" "prefetch" "$VMSTAT 1" "vmstat"
+    "$MPSTAT 1" "mpstat" "$IOSTAT -xcnz 1" "iostat"
+    "dtrace -s $PERF_SCRIPTS/profile.d" "profile" "kstat zfs:0 1" "kstat")
+
+log_note "Sequential cached reads with $PERF_RUNTYPE settings"
+do_fio_run sequential_reads.fio $FALSE $FALSE
+log_pass "Measure IO stats during sequential cached read load"
diff --git a/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_arc_cached_clone.ksh b/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_arc_cached_clone.ksh
new file mode 100644
index 0000000..f389b3a
--- /dev/null
+++ b/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_arc_cached_clone.ksh
@@ -0,0 +1,94 @@
+#!/usr/bin/ksh
+
+#
+# This file and its contents are supplied under the terms of the
+# Common Development and Distribution License ("CDDL"), version 1.0.
+# You may only use this file in accordance with the terms of version
+# 1.0 of the CDDL.
+#
+# A full copy of the text of the CDDL should have accompanied this
+# source.  A copy of the CDDL is also available via the Internet at
+# http://www.illumos.org/license/CDDL.
+#
+
+#
+# Copyright (c) 2015 by Delphix. All rights reserved.
+#
+
+#
+# Description:
+# Trigger fio runs using the sequential_reads job file. The number of runs and
+# data collected is determined by the PERF_* variables. See do_fio_run for
+# details about these variables.
+#
+# The files to read from are created prior to the first fio run, and used
+# for all fio runs. This test will exercise cached read performance from
+# a clone filesystem. The data is initially cached in the ARC and then
+# a snapshot and clone are created. All the performance runs are then
+# initiated against the clone filesystem to exercise the performance of
+# reads when the ARC has to create another buffer from a different dataset.
+# It will also exercise the need to evict the duplicate buffer once the last
+# reference on that buffer is released.
+#
+
+. $STF_SUITE/include/libtest.shlib
+. $STF_SUITE/tests/perf/perf.shlib
+
+function cleanup
+{
+	log_must $ZFS destroy $TESTFS
+}
+
+log_assert "Measure IO stats during sequential read load"
+log_onexit cleanup
+
+export TESTFS=$PERFPOOL/testfs
+recreate_perfpool
+log_must $ZFS create $PERF_FS_OPTS $TESTFS
+
+# Make sure the working set can be cached in the arc. Aim for 1/2 of arc.
+export TOTAL_SIZE=$(($(get_max_arc_size) / 2))
+
+# Variables for use by fio.
+if [[ -n $PERF_REGRESSION_WEEKLY ]]; then
+	export PERF_RUNTIME=${PERF_RUNTIME:-$PERF_RUNTIME_WEEKLY}
+	export PERF_RUNTYPE=${PERF_RUNTYPE:-'weekly'}
+	export PERF_NTHREADS=${PERF_NTHREADS:-'16 64'}
+	export PERF_SYNC_TYPES=${PERF_SYNC_TYPES:-'1'}
+	export PERF_IOSIZES=${PERF_IOSIZES:-'64k 128k 1m'}
+elif [[ -n $PERF_REGRESSION_NIGHTLY ]]; then
+	export PERF_RUNTIME=${PERF_RUNTIME:-$PERF_RUNTIME_NIGHTLY}
+	export PERF_RUNTYPE=${PERF_RUNTYPE:-'nightly'}
+	export PERF_NTHREADS=${PERF_NTHREADS:-'64 128'}
+	export PERF_SYNC_TYPES=${PERF_SYNC_TYPES:-'1'}
+	export PERF_IOSIZES=${PERF_IOSIZES:-'128k 1m'}
+fi
+
+# Layout the files to be used by the read tests. Create as many files as the
+# largest number of threads. An fio run with fewer threads will use a subset
+# of the available files.
+export NUMJOBS=$(get_max $PERF_NTHREADS)
+export FILE_SIZE=$((TOTAL_SIZE / NUMJOBS))
+log_must $FIO $FIO_SCRIPTS/mkfiles.fio
+
+log_note "Creating snapshot, $TESTSNAP, of $TESTFS"
+create_snapshot $TESTFS $TESTSNAP
+log_note "Creating clone, $PERFPOOL/$TESTCLONE, from $TESTFS@$TESTSNAP"
+create_clone $TESTFS@$TESTSNAP $PERFPOOL/$TESTCLONE
+
+#
+# Reset the TESTFS to point to the clone
+#
+export TESTFS=$PERFPOOL/$TESTCLONE
+
+# Set up the scripts and output files that will log performance data.
+lun_list=$(pool_to_lun_list $PERFPOOL)
+log_note "Collecting backend IO stats with lun list $lun_list"
+export collect_scripts=("$PERF_SCRIPTS/io.d $PERFPOOL $lun_list 1" "io"
+    "$PERF_SCRIPTS/prefetch_io.d $PERFPOOL 1" "prefetch" "$VMSTAT 1" "vmstat"
+    "$MPSTAT 1" "mpstat" "$IOSTAT -xcnz 1" "iostat"
+    "dtrace -s $PERF_SCRIPTS/profile.d" "profile" "kstat zfs:0 1" "kstat")
+
+log_note "Sequential cached reads from $TESTFS with $PERF_RUNTYPE settings"
+do_fio_run sequential_reads.fio $FALSE $FALSE
+log_pass "Measure IO stats during sequential cached read load"
diff --git a/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_cached.ksh b/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_cached.ksh
deleted file mode 100644
index 70cddd4..0000000
--- a/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_cached.ksh
+++ /dev/null
@@ -1,77 +0,0 @@
-#!/usr/bin/ksh
-
-#
-# This file and its contents are supplied under the terms of the
-# Common Development and Distribution License ("CDDL"), version 1.0.
-# You may only use this file in accordance with the terms of version
-# 1.0 of the CDDL.
-#
-# A full copy of the text of the CDDL should have accompanied this
-# source.  A copy of the CDDL is also available via the Internet at
-# http://www.illumos.org/license/CDDL.
-#
-
-#
-# Copyright (c) 2015 by Delphix. All rights reserved.
-#
-
-#
-# Description:
-# Trigger fio runs using the sequential_reads job file. The number of runs and
-# data collected is determined by the PERF_* variables. See do_fio_run for
-# details about these variables.
-#
-# The files to read from are created prior to the first fio run, and used
-# for all fio runs. The ARC is not cleared to ensure that all data is cached.
-#
-
-. $STF_SUITE/include/libtest.shlib
-. $STF_SUITE/tests/perf/perf.shlib
-
-function cleanup
-{
-	log_must $ZFS destroy $TESTFS
-}
-
-log_assert "Measure IO stats during sequential read load"
-log_onexit cleanup
-
-export TESTFS=$PERFPOOL/testfs
-recreate_perfpool
-log_must $ZFS create $PERF_FS_OPTS $TESTFS
-
-# Make sure the working set can be cached in the arc. Aim for 1/2 of arc.
-export TOTAL_SIZE=$(($(get_max_arc_size) / 2))
-
-# Variables for use by fio.
-if [[ -n $PERF_REGRESSION_WEEKLY ]]; then
-	export PERF_RUNTIME=${PERF_RUNTIME:-$PERF_RUNTIME_WEEKLY}
-	export PERF_RUNTYPE=${PERF_RUNTYPE:-'weekly'}
-	export PERF_NTHREADS=${PERF_NTHREADS:-'16 64'}
-	export PERF_SYNC_TYPES=${PERF_SYNC_TYPES:-'1'}
-	export PERF_IOSIZES=${PERF_IOSIZES:-'64k 128k 1m'}
-elif [[ -n $PERF_REGRESSION_NIGHTLY ]]; then
-	export PERF_RUNTIME=${PERF_RUNTIME:-$PERF_RUNTIME_NIGHTLY}
-	export PERF_RUNTYPE=${PERF_RUNTYPE:-'nightly'}
-	export PERF_NTHREADS=${PERF_NTHREADS:-'64 128'}
-	export PERF_SYNC_TYPES=${PERF_SYNC_TYPES:-'1'}
-	export PERF_IOSIZES=${PERF_IOSIZES:-'128k 1m'}
-fi
-
-# Layout the files to be used by the read tests. Create as many files as the
-# largest number of threads. An fio run with fewer threads will use a subset
-# of the available files.
-export NUMJOBS=$(get_max $PERF_NTHREADS)
-export FILE_SIZE=$((TOTAL_SIZE / NUMJOBS))
-log_must $FIO $FIO_SCRIPTS/mkfiles.fio
-
-# Set up the scripts and output files that will log performance data.
-lun_list=$(pool_to_lun_list $PERFPOOL)
-log_note "Collecting backend IO stats with lun list $lun_list"
-export collect_scripts=("$PERF_SCRIPTS/io.d $PERFPOOL $lun_list 1" "io"
-    "$PERF_SCRIPTS/prefetch_io.d $PERFPOOL 1" "prefetch" "$VMSTAT 1" "vmstat"
-    "$MPSTAT 1" "mpstat" "$IOSTAT -xcnz 1" "iostat")
-
-log_note "Sequential cached reads with $PERF_RUNTYPE settings"
-do_fio_run sequential_reads.fio $FALSE $FALSE
-log_pass "Measure IO stats during sequential cached read load"
diff --git a/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_cached_clone.ksh b/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_cached_clone.ksh
deleted file mode 100644
index c4790f1..0000000
--- a/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_cached_clone.ksh
+++ /dev/null
@@ -1,93 +0,0 @@
-#!/usr/bin/ksh
-
-#
-# This file and its contents are supplied under the terms of the
-# Common Development and Distribution License ("CDDL"), version 1.0.
-# You may only use this file in accordance with the terms of version
-# 1.0 of the CDDL.
-#
-# A full copy of the text of the CDDL should have accompanied this
-# source.  A copy of the CDDL is also available via the Internet at
-# http://www.illumos.org/license/CDDL.
-#
-
-#
-# Copyright (c) 2015 by Delphix. All rights reserved.
-#
-
-#
-# Description:
-# Trigger fio runs using the sequential_reads job file. The number of runs and
-# data collected is determined by the PERF_* variables. See do_fio_run for
-# details about these variables.
-#
-# The files to read from are created prior to the first fio run, and used
-# for all fio runs. This test will exercise cached read performance from
-# a clone filesystem. The data is initially cached in the ARC and then
-# a snapshot and clone are created. All the performance runs are then
-# initiated against the clone filesystem to exercise the performance of
-# reads when the ARC has to create another buffer from a different dataset.
-# It will also exercise the need to evict the duplicate buffer once the last
-# reference on that buffer is released.
-#
-
-. $STF_SUITE/include/libtest.shlib
-. $STF_SUITE/tests/perf/perf.shlib
-
-function cleanup
-{
-	log_must $ZFS destroy $TESTFS
-}
-
-log_assert "Measure IO stats during sequential read load"
-log_onexit cleanup
-
-export TESTFS=$PERFPOOL/testfs
-recreate_perfpool
-log_must $ZFS create $PERF_FS_OPTS $TESTFS
-
-# Make sure the working set can be cached in the arc. Aim for 1/2 of arc.
-export TOTAL_SIZE=$(($(get_max_arc_size) / 2))
-
-# Variables for use by fio.
-if [[ -n $PERF_REGRESSION_WEEKLY ]]; then
-	export PERF_RUNTIME=${PERF_RUNTIME:-$PERF_RUNTIME_WEEKLY}
-	export PERF_RUNTYPE=${PERF_RUNTYPE:-'weekly'}
-	export PERF_NTHREADS=${PERF_NTHREADS:-'16 64'}
-	export PERF_SYNC_TYPES=${PERF_SYNC_TYPES:-'1'}
-	export PERF_IOSIZES=${PERF_IOSIZES:-'64k 128k 1m'}
-elif [[ -n $PERF_REGRESSION_NIGHTLY ]]; then
-	export PERF_RUNTIME=${PERF_RUNTIME:-$PERF_RUNTIME_NIGHTLY}
-	export PERF_RUNTYPE=${PERF_RUNTYPE:-'nightly'}
-	export PERF_NTHREADS=${PERF_NTHREADS:-'64 128'}
-	export PERF_SYNC_TYPES=${PERF_SYNC_TYPES:-'1'}
-	export PERF_IOSIZES=${PERF_IOSIZES:-'128k 1m'}
-fi
-
-# Layout the files to be used by the read tests. Create as many files as the
-# largest number of threads. An fio run with fewer threads will use a subset
-# of the available files.
-export NUMJOBS=$(get_max $PERF_NTHREADS)
-export FILE_SIZE=$((TOTAL_SIZE / NUMJOBS))
-log_must $FIO $FIO_SCRIPTS/mkfiles.fio
-
-log_note "Creating snapshot, $TESTSNAP, of $TESTFS"
-create_snapshot $TESTFS $TESTSNAP
-log_note "Creating clone, $PERFPOOL/$TESTCLONE, from $TESTFS@$TESTSNAP"
-create_clone $TESTFS@$TESTSNAP $PERFPOOL/$TESTCLONE
-
-#
-# Reset the TESTFS to point to the clone
-#
-export TESTFS=$PERFPOOL/$TESTCLONE
-
-# Set up the scripts and output files that will log performance data.
-lun_list=$(pool_to_lun_list $PERFPOOL)
-log_note "Collecting backend IO stats with lun list $lun_list"
-export collect_scripts=("$PERF_SCRIPTS/io.d $PERFPOOL $lun_list 1" "io"
-    "$PERF_SCRIPTS/prefetch_io.d $PERFPOOL 1" "prefetch" "$VMSTAT 1" "vmstat"
-    "$MPSTAT 1" "mpstat" "$IOSTAT -xcnz 1" "iostat")
-
-log_note "Sequential cached reads from $TESTFS with $PERF_RUNTYPE settings"
-do_fio_run sequential_reads.fio $FALSE $FALSE
-log_pass "Measure IO stats during sequential cached read load"
diff --git a/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_dbuf_cached.ksh b/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_dbuf_cached.ksh
new file mode 100644
index 0000000..e3648be
--- /dev/null
+++ b/usr/src/test/zfs-tests/tests/perf/regression/sequential_reads_dbuf_cached.ksh
@@ -0,0 +1,82 @@
+#!/usr/bin/ksh
+
+#
+# This file and its contents are supplied under the terms of the
+# Common Development and Distribution License ("CDDL"), version 1.0.
+# You may only use this file in accordance with the terms of version
+# 1.0 of the CDDL.
+#
+# A full copy of the text of the CDDL should have accompanied this
+# source.  A copy of the CDDL is also available via the Internet at
+# http://www.illumos.org/license/CDDL.
+#
+
+#
+# Copyright (c) 2016 by Delphix. All rights reserved.
+#
+
+#
+# Description:
+# Trigger fio runs using the sequential_reads job file. The number of runs and
+# data collected is determined by the PERF_* variables. See do_fio_run for
+# details about these variables.
+#
+# The files to read from are created prior to the first fio run, and used
+# for all fio runs. The ARC is not cleared to ensure that all data is cached.
+#
+# This is basically a copy of the sequential_reads_cached test case, but with
+# a smaller dateset so that we can fit everything into the decompressed, linear
+# space in the dbuf cache.
+#
+
+. $STF_SUITE/include/libtest.shlib
+. $STF_SUITE/tests/perf/perf.shlib
+
+function cleanup
+{
+	log_must $ZFS destroy $TESTFS
+}
+
+log_assert "Measure IO stats during sequential read load"
+log_onexit cleanup
+
+export TESTFS=$PERFPOOL/testfs
+recreate_perfpool
+log_must $ZFS create $PERF_FS_OPTS $TESTFS
+
+# Ensure the working set can be cached in the dbuf cache.
+export TOTAL_SIZE=$(($(get_max_dbuf_cache_size) * 3 / 4))
+
+# Variables for use by fio.
+if [[ -n $PERF_REGRESSION_WEEKLY ]]; then
+	export PERF_RUNTIME=${PERF_RUNTIME:-$PERF_RUNTIME_WEEKLY}
+	export PERF_RUNTYPE=${PERF_RUNTYPE:-'weekly'}
+	export PERF_NTHREADS=${PERF_NTHREADS:-'16 64'}
+	export PERF_SYNC_TYPES=${PERF_SYNC_TYPES:-'1'}
+	export PERF_IOSIZES=${PERF_IOSIZES:-'64k'}
+elif [[ -n $PERF_REGRESSION_NIGHTLY ]]; then
+	export PERF_RUNTIME=${PERF_RUNTIME:-$PERF_RUNTIME_NIGHTLY}
+	export PERF_RUNTYPE=${PERF_RUNTYPE:-'nightly'}
+	export PERF_NTHREADS=${PERF_NTHREADS:-'64'}
+	export PERF_SYNC_TYPES=${PERF_SYNC_TYPES:-'1'}
+	export PERF_IOSIZES=${PERF_IOSIZES:-'64k'}
+fi
+
+# Layout the files to be used by the read tests. Create as many files as the
+# largest number of threads. An fio run with fewer threads will use a subset
+# of the available files.
+export NUMJOBS=$(get_max $PERF_NTHREADS)
+export FILE_SIZE=$((TOTAL_SIZE / NUMJOBS))
+log_must $FIO $FIO_SCRIPTS/mkfiles.fio
+
+# Set up the scripts and output files that will log performance data.
+lun_list=$(pool_to_lun_list $PERFPOOL)
+log_note "Collecting backend IO stats with lun list $lun_list"
+export collect_scripts=("dtrace -s $PERF_SCRIPTS/io.d $PERFPOOL $lun_list 1"
+    "io" "dtrace -Cs $PERF_SCRIPTS/prefetch_io.d $PERFPOOL 1" "prefetch"
+    "$VMSTAT 1" "vmstat" "$MPSTAT 1" "mpstat" "$IOSTAT -xcnz 1" "iostat"
+    "dtrace -s $PERF_SCRIPTS/profile.d" "profile" "kstat zfs:0 1" "kstat")
+
+log_note "Sequential cached reads with $PERF_RUNTYPE settings"
+do_fio_run sequential_reads.fio $FALSE $FALSE
+log_pass "Measure IO stats during sequential cached read load"
diff --git a/usr/src/test/zfs-tests/tests/perf/regression/sequential_writes.ksh b/usr/src/test/zfs-tests/tests/perf/regression/sequential_writes.ksh
index 5e587e8..022dd0f 100644
--- a/usr/src/test/zfs-tests/tests/perf/regression/sequential_writes.ksh
+++ b/usr/src/test/zfs-tests/tests/perf/regression/sequential_writes.ksh
@@ -62,7 +62,8 @@ fi
 lun_list=$(pool_to_lun_list $PERFPOOL)
 log_note "Collecting backend IO stats with lun list $lun_list"
 export collect_scripts=("$PERF_SCRIPTS/io.d $PERFPOOL $lun_list 1" "io"
-    "$VMSTAT 1" "vmstat" "$MPSTAT 1" "mpstat" "$IOSTAT -xcnz 1" "iostat")
+    "$VMSTAT 1" "vmstat" "$MPSTAT 1" "mpstat" "$IOSTAT -xcnz 1" "iostat"
+    "dtrace -s $PERF_SCRIPTS/profile.d" "profile" "kstat zfs:0 1" "kstat")
 
 log_note "Sequential writes with $PERF_RUNTYPE settings"
 do_fio_run sequential_writes.fio $TRUE $FALSE
diff --git a/usr/src/test/zfs-tests/tests/perf/scripts/profile.d b/usr/src/test/zfs-tests/tests/perf/scripts/profile.d
new file mode 100644
index 0000000..e7fbd1f
--- /dev/null
+++ b/usr/src/test/zfs-tests/tests/perf/scripts/profile.d
@@ -0,0 +1,37 @@
+#!/usr/sbin/dtrace -s
+
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright (c) 2016 by Delphix. All rights reserved.
+ */
+
+#pragma D option stackframes=100
+
+/*
+ * @stacks: The number of times a stack has been recorded
+ */
+
+profile-997
+/ arg0 /
+{
+	@stacks[stack()] = count();
+}
+
+ERROR
+{
+    trace(arg1);
+    trace(arg2);
+    trace(arg3);
+    trace(arg4);
+    trace(arg5);
+}
1.8.3.1
