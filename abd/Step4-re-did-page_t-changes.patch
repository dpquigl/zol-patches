From 0c63bf7e22aefba2f831127bcd10ef1a595b4633 Mon Sep 17 00:00:00 2001
From: Dan Kimmel <dan.kimmel@delphix.com>
Date: Fri, 27 May 2016 20:04:11 -0700
Subject: [PATCH 6/6] re-did page_t changes

---
 usr/src/uts/common/fs/zfs/abd.c     | 201 ++++++++++++++++++++++++------------
 usr/src/uts/common/fs/zfs/sys/abd.h |   8 +-
 2 files changed, 141 insertions(+), 68 deletions(-)

diff --git a/usr/src/uts/common/fs/zfs/abd.c b/usr/src/uts/common/fs/zfs/abd.c
index 82b228d..d037fc9 100644
--- a/usr/src/uts/common/fs/zfs/abd.c
+++ b/usr/src/uts/common/fs/zfs/abd.c
@@ -59,10 +59,30 @@
  *                                      +----------------->| chunk N-1 |
  *                                                         +-----------+
  *
- * Using a large proportion of scattered ABDs decreases ARC fragmentation since
- * when we are at the limit of allocatable space, using equal-size chunks will
- * allow us to quickly reclaim enough space for a new large allocation (assuming
- * it is also scattered).
+ * Linear buffers act exactly like normal buffers and are always mapped into the
+ * kernel's virtual memory space, while scattered ABD data chunks are allocated
+ * as physical pages and then mapped in only while they are actually being
+ * accessed through one of the abd_* library functions. Using scattered ABDs
+ * provides several benefits:
+ *
+ *  (1) They avoid use of kmem_*, preventing performance problems where running
+ *      kmem_reap on very large memory systems never finishes and causes
+ *      constant TLB shootdowns.
+ *
+ *  (2) Fragmentation is less of an issue since when we are at the limit of
+ *      allocatable space, we won't have to search around for a long free
+ *      hole in the VA space for large ARC allocations. Each chunk is mapped in
+ *      individually, so even if we weren't using segkpm (see next point) we
+ *      wouldn't need to worry about finding a contiguous address range.
+ *
+ *  (3) Use of segkpm will avoid the need for map / unmap / TLB shootdown costs
+ *      on each ABD access. (If segkpm isn't available then we use all linear
+ *      ABDs to avoid this penalty.) See seg_kpm.c for more details.
+ *
+ * It is possible to make all ABDs linear by setting zfs_abd_scatter_enabled to
+ * B_FALSE. However, it is not possible to use scattered ABDs if segkpm is not
+ * available, which is the case on all 32-bit systems and any 64-bit systems
+ * where kpm_enable is turned off.
  *
  * In addition to directly allocating a linear or scattered ABD, it is also
  * possible to create an ABD by requesting the "sub-ABD" starting at an offset
@@ -99,6 +119,7 @@
 #include <sys/zio.h>
 #include <sys/zfs_context.h>
 #include <sys/zfs_znode.h>
+#include <vm/seg_kpm.h>
 
 typedef struct abd_stats {
 	kstat_named_t abdstat_struct_size;
@@ -142,59 +163,90 @@ static abd_stats_t abd_stats = {
 #define	ABDSTAT_BUMP(stat)	ABDSTAT_INCR(stat, 1)
 #define	ABDSTAT_BUMPDOWN(stat)	ABDSTAT_INCR(stat, -1)
 
-/*
- * It is possible to make all future ABDs be linear by setting this to B_FALSE.
- * Otherwise, ABDs are allocated scattered by default unless the caller uses
- * abd_alloc_linear().
- */
+/* see block comment above for description */
 boolean_t zfs_abd_scatter_enabled = B_TRUE;
 
-/*
- * The size of the chunks ABD allocates. Because the sizes allocated from the
- * kmem_cache can't change, this tunable can only be modified at boot. Changing
- * it at runtime would cause ABD iteration to work incorrectly for ABDs which
- * were allocated with the old size, so a safeguard has been put in place which
- * will cause the machine to panic if you change it and try to access the data
- * within a scattered ABD.
- */
-size_t zfs_abd_chunk_size = 1024;
-
-#ifdef _KERNEL
-extern vmem_t *zio_alloc_arena;
-#endif
-
-static kmem_cache_t *abd_chunk_cache;
 static kstat_t *abd_ksp;
 
-static void *
+#ifdef _KERNEL
+
+static vmem_t *abd_offset_arena;
+
+static struct page *
 abd_alloc_chunk()
 {
-	void *c = kmem_cache_alloc(abd_chunk_cache, KM_PUSHPAGE);
+	static struct seg tmp_seg;
+	struct page *c = NULL;
+
+	while (c == NULL) {
+		/* ensure we have memory to allocate */
+		VERIFY3U(page_resv(1, KM_SLEEP), ==, 1);
+
+		/*
+		 * Get an unique offset value for this page and use
+		 * segkpm_create_va to create an intelligent address
+		 * based on the offset.
+		 */
+		u_offset_t offset = (u_offset_t)(uintptr_t)vmem_alloc(
+		    abd_offset_arena, 1, VM_BESTFIT | VM_SLEEP);
+		caddr_t base = segkpm_create_va(offset);
+
+		c = page_create_va(&zvp, offset, MMU_PAGESIZE,
+		    PG_EXCL | PG_WAIT | PG_PUSHPAGE, &tmp_seg, base);
+		if (c != NULL) {
+			page_io_unlock(c);
+			page_downgrade(c);
+		} else {
+			vmem_free(abd_offset_arena,
+			    (void *)(uintptr_t)offset, 1);
+			page_unresv(1);
+		}
+	}
 	ASSERT3P(c, !=, NULL);
 	return (c);
 }
 
 static void
-abd_free_chunk(void *c)
+abd_free_chunk(struct page *c)
 {
-	kmem_cache_free(abd_chunk_cache, c);
+	u_offset_t off = c->p_offset;
+
+	ASSERT(PAGE_SHARED(c));
+	vmem_free(abd_offset_arena, (void *)(uintptr_t)off, 1);
+
+	if (!page_tryupgrade(c)) {
+		page_unlock(c);
+		c = page_lookup(&zvp, off, SE_EXCL);
+		if (c == NULL)
+			panic("ABD chunk not found");
+	}
+
+	/* We pass 0 for "dontfree" -- therefore this will free the page. */
+	page_destroy(c, 0);
+	page_unresv(1);
+}
+
+static void *
+abd_map_chunk(struct page *c)
+{
+	/*
+	 * Use of segkpm means we don't care if this is mapped S_READ or S_WRITE
+	 * but S_WRITE is conceptually more accurate.
+	 */
+	return ((void *)zfs_map_page(c, S_WRITE));
+}
+
+static void
+abd_unmap_chunk(struct page *c, void *addr)
+{
+	zfs_unmap_page(c, addr);
 }
 
 void
 abd_init(void)
 {
-	vmem_t *data_alloc_arena = NULL;
-
-#ifdef _KERNEL
-	data_alloc_arena = zio_alloc_arena;
-#endif
-
-	/*
-	 * Since ABD chunks do not appear in crash dumps, we pass KMC_NOTOUCH
-	 * so that no allocator metadata is stored with the buffers.
-	 */
-	abd_chunk_cache = kmem_cache_create("abd_chunk", zfs_abd_chunk_size, 0,
-	    NULL, NULL, NULL, NULL, data_alloc_arena, KMC_NOTOUCH);
+	abd_offset_arena = vmem_create("abd_offset", (void *)1, SIZE_MAX, 1,
+	    NULL, NULL, NULL, 0, VM_SLEEP | VMC_IDENTIFIER);
 
 	abd_ksp = kstat_create("zfs", 0, "abdstats", "misc", KSTAT_TYPE_NAMED,
 	    sizeof (abd_stats) / sizeof (kstat_named_t), KSTAT_FLAG_VIRTUAL);
@@ -212,14 +264,35 @@ abd_fini(void)
 		abd_ksp = NULL;
 	}
 
-	kmem_cache_destroy(abd_chunk_cache);
-	abd_chunk_cache = NULL;
+	vmem_destroy(abd_offset_arena);
 }
 
+#else
+
+struct page;
+#define	kpm_enable			1
+#define	abd_alloc_chunk() \
+	((struct page *)kmem_alloc(PAGESIZE, KM_SLEEP))
+#define	abd_free_chunk(chunk) 		kmem_free(chunk, PAGESIZE)
+#define	abd_map_chunk(chunk)		((void *)chunk)
+#define	abd_unmap_chunk(chunk, addr)	((void)0)
+
+void
+abd_init(void)
+{
+}
+
+void
+abd_fini(void)
+{
+}
+
+#endif /* _KERNEL */
+
 static inline size_t
 abd_chunkcnt_for_bytes(size_t size)
 {
-	return (P2ROUNDUP(size, zfs_abd_chunk_size) / zfs_abd_chunk_size);
+	return (P2ROUNDUP(size, PAGESIZE) / PAGESIZE);
 }
 
 static inline size_t
@@ -242,8 +315,7 @@ abd_verify(abd_t *abd)
 	if (abd_is_linear(abd)) {
 		ASSERT3P(abd->abd_u.abd_linear.abd_buf, !=, NULL);
 	} else {
-		ASSERT3U(abd->abd_u.abd_scatter.abd_offset, <,
-		    zfs_abd_chunk_size);
+		ASSERT3U(abd->abd_u.abd_scatter.abd_offset, <, PAGESIZE);
 		size_t n = abd_scatter_chunkcnt(abd);
 		for (int i = 0; i < n; i++) {
 			ASSERT3P(
@@ -296,7 +368,7 @@ abd_alloc(size_t size, boolean_t is_metadata)
 	refcount_create(&abd->abd_children);
 
 	abd->abd_u.abd_scatter.abd_offset = 0;
-	abd->abd_u.abd_scatter.abd_chunk_size = zfs_abd_chunk_size;
+	abd->abd_u.abd_scatter.abd_chunk_size = PAGESIZE;
 
 	for (int i = 0; i < n; i++) {
 		void *c = abd_alloc_chunk();
@@ -306,8 +378,7 @@ abd_alloc(size_t size, boolean_t is_metadata)
 
 	ABDSTAT_BUMP(abdstat_scatter_cnt);
 	ABDSTAT_INCR(abdstat_scatter_data_size, size);
-	ABDSTAT_INCR(abdstat_scatter_chunk_waste,
-	    n * zfs_abd_chunk_size - size);
+	ABDSTAT_INCR(abdstat_scatter_chunk_waste, n * PAGESIZE - size);
 
 	return (abd);
 }
@@ -323,8 +394,7 @@ abd_free_scatter(abd_t *abd)
 	refcount_destroy(&abd->abd_children);
 	ABDSTAT_BUMPDOWN(abdstat_scatter_cnt);
 	ABDSTAT_INCR(abdstat_scatter_data_size, -(int)abd->abd_size);
-	ABDSTAT_INCR(abdstat_scatter_chunk_waste,
-	    abd->abd_size - n * zfs_abd_chunk_size);
+	ABDSTAT_INCR(abdstat_scatter_chunk_waste, abd->abd_size - n * PAGESIZE);
 
 	abd_free_struct(abd);
 }
@@ -452,7 +522,7 @@ abd_get_offset(abd_t *sabd, size_t off)
 	} else {
 		size_t new_offset = sabd->abd_u.abd_scatter.abd_offset + off;
 		size_t chunkcnt = abd_scatter_chunkcnt(sabd) -
-		    (new_offset / zfs_abd_chunk_size);
+		    (new_offset / PAGESIZE);
 
 		abd = abd_alloc_struct(chunkcnt);
 
@@ -463,14 +533,12 @@ abd_get_offset(abd_t *sabd, size_t off)
 		 */
 		abd->abd_flags = 0;
 
-		abd->abd_u.abd_scatter.abd_offset =
-		    new_offset % zfs_abd_chunk_size;
-		abd->abd_u.abd_scatter.abd_chunk_size = zfs_abd_chunk_size;
+		abd->abd_u.abd_scatter.abd_offset = new_offset % PAGESIZE;
+		abd->abd_u.abd_scatter.abd_chunk_size = PAGESIZE;
 
 		/* Copy the scatterlist starting at the correct offset */
 		(void) memcpy(&abd->abd_u.abd_scatter.abd_chunks,
-		    &sabd->abd_u.abd_scatter.abd_chunks[new_offset /
-		    zfs_abd_chunk_size],
+		    &sabd->abd_u.abd_scatter.abd_chunks[new_offset / PAGESIZE],
 		    chunkcnt * sizeof (void *));
 	}
 
@@ -648,7 +716,7 @@ abd_iter_scatter_chunk_offset(struct abd_iter *aiter)
 {
 	ASSERT(!abd_is_linear(aiter->iter_abd));
 	return ((aiter->iter_abd->abd_u.abd_scatter.abd_offset +
-	    aiter->iter_pos) % zfs_abd_chunk_size);
+	    aiter->iter_pos) % PAGESIZE);
 }
 
 static inline size_t
@@ -656,7 +724,7 @@ abd_iter_scatter_chunk_index(struct abd_iter *aiter)
 {
 	ASSERT(!abd_is_linear(aiter->iter_abd));
 	return ((aiter->iter_abd->abd_u.abd_scatter.abd_offset +
-	    aiter->iter_pos) / zfs_abd_chunk_size);
+	    aiter->iter_pos) / PAGESIZE);
 }
 
 /*
@@ -703,10 +771,6 @@ abd_iter_map(struct abd_iter *aiter)
 	ASSERT3P(aiter->iter_mapaddr, ==, NULL);
 	ASSERT0(aiter->iter_mapsize);
 
-	/* Panic if someone has changed zfs_abd_chunk_size */
-	IMPLY(!abd_is_linear(aiter->iter_abd), zfs_abd_chunk_size ==
-	    aiter->iter_abd->abd_u.abd_scatter.abd_chunk_size);
-
 	/* There's nothing left to iterate over, so do nothing */
 	if (aiter->iter_pos == aiter->iter_abd->abd_size)
 		return;
@@ -718,8 +782,9 @@ abd_iter_map(struct abd_iter *aiter)
 	} else {
 		size_t index = abd_iter_scatter_chunk_index(aiter);
 		offset = abd_iter_scatter_chunk_offset(aiter);
-		aiter->iter_mapsize = zfs_abd_chunk_size - offset;
-		paddr = aiter->iter_abd->abd_u.abd_scatter.abd_chunks[index];
+		aiter->iter_mapsize = PAGESIZE - offset;
+		paddr = abd_map_chunk(
+		    aiter->iter_abd->abd_u.abd_scatter.abd_chunks[index]);
 	}
 	aiter->iter_mapaddr = (char *)paddr + offset;
 }
@@ -735,6 +800,14 @@ abd_iter_unmap(struct abd_iter *aiter)
 	if (aiter->iter_pos == aiter->iter_abd->abd_size)
 		return;
 
+	if (!abd_is_linear(aiter->iter_abd)) {
+		/* LINTED E_FUNC_SET_NOT_USED */
+		size_t index = abd_iter_scatter_chunk_index(aiter);
+		abd_unmap_chunk(
+		    aiter->iter_abd->abd_u.abd_scatter.abd_chunks[index],
+		    aiter->iter_mapaddr);
+        }
+
 	ASSERT3P(aiter->iter_mapaddr, !=, NULL);
 	ASSERT3U(aiter->iter_mapsize, >, 0);
 
diff --git a/usr/src/uts/common/fs/zfs/sys/abd.h b/usr/src/uts/common/fs/zfs/sys/abd.h
index 8a60d1b..b5843f9 100644
--- a/usr/src/uts/common/fs/zfs/sys/abd.h
+++ b/usr/src/uts/common/fs/zfs/sys/abd.h
@@ -51,12 +51,12 @@ typedef struct abd {
 	refcount_t	abd_children;
 	union {
 		struct abd_scatter {
-			uint_t	abd_offset;
-			uint_t	abd_chunk_size;
-			void	*abd_chunks[];
+			uint_t		abd_offset;
+			uint_t		abd_chunk_size;
+			struct page	*abd_chunks[];
 		} abd_scatter;
 		struct abd_linear {
-			void	*abd_buf;
+			void		*abd_buf;
 		} abd_linear;
 	} abd_u;
 } abd_t;
-- 
1.8.3.1

